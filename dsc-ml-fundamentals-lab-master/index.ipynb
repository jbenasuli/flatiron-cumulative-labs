{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Fundamentals - Cumulative Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this cumulative lab, you will work through an end-to-end machine learning workflow, focusing on the fundamental concepts of machine learning theory and processes. The main emphasis is on modeling theory (not EDA or preprocessing), so we will skip over some of the data visualization and data preparation steps that you would take in an actual modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Recall the purpose of, and practice performing, a train-test split\n",
    "* Recall the difference between bias and variance\n",
    "* Practice identifying bias and variance in model performance\n",
    "* Practice applying strategies to minimize bias and variance\n",
    "* Practice selecting a final model and evaluating it on a holdout set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task: Build a Model to Predict Blood Pressure\n",
    "\n",
    "![stethoscope sitting on a case](images/stethoscope.jpg)\n",
    "\n",
    "<span>Photo by <a href=\"https://unsplash.com/@marceloleal80?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Marcelo Leal</a> on <a href=\"https://unsplash.com/s/photos/blood-pressure?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business and Data Understanding\n",
    "\n",
    "Hypertension (high blood pressure) is a treatable condition, but measuring blood pressure requires specialized equipment that most people do not have at home.\n",
    "\n",
    "The question, then, is ***can we predict blood pressure using just a scale and a tape measure***? These measuring tools, which individuals are more likely to have at home, might be able to flag individuals with an increased risk of hypertension.\n",
    "\n",
    "[Researchers in Brazil](https://doi.org/10.1155/2014/637635) collected data from several hundred college students in order to answer this question. We will be specifically using the data they collected from female students.\n",
    "\n",
    "The measurements we have are:\n",
    "\n",
    "* Age (age in years)\n",
    "* BMI (body mass index, a ratio of weight to height)\n",
    "* WC (waist circumference in centimeters)\n",
    "* HC (hip circumference in centimeters)\n",
    "* WHR (waist-hip ratio)\n",
    "* SBP (systolic blood pressure)\n",
    "\n",
    "The chart below describes various blood pressure values:\n",
    "\n",
    "<a title=\"Ian Furst, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Hypertension_ranges_chart.png\"><img width=\"512\" alt=\"Hypertension ranges chart\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8b/Hypertension_ranges_chart.png/512px-Hypertension_ranges_chart.png\"></a>\n",
    "\n",
    "### Requirements\n",
    "\n",
    "#### 1. Perform a Train-Test Split\n",
    "\n",
    "Load the data into a dataframe using pandas, separate the features (`X`) from the target (`y`), and use the `train_test_split` function to separate data into training and test sets.\n",
    "\n",
    "#### 2. Build and Evaluate a First Simple Model\n",
    "\n",
    "Using the `LinearRegression` model and `mean_squared_error` function from scikit-learn, build and evaluate a simple linear regression model using the training data. Also, use `cross_val_score` to simulate unseen data, without actually using the holdout test set.\n",
    "\n",
    "#### 3. Use `PolynomialFeatures` to Reduce Underfitting\n",
    "\n",
    "Apply a `PolynomialFeatures` transformer to give the model more ability to pick up on information from the training data. Test out different polynomial degrees until you have a model that is perfectly fit to the training data.\n",
    "\n",
    "#### 4. Use Regularization to Reduce Overfitting\n",
    "\n",
    "Instead of a basic `LinearRegression`, use a `Ridge` regression model to apply regularization to the overfit model. In order to do this you will need to scale the data. Test out different regularization penalties to find the best model.\n",
    "\n",
    "#### 5. Evaluate a Final Model on the Test Set\n",
    "\n",
    "Preprocess `X_test` and `y_test` appropriately in order to evaluate the performance of your final model on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perform a Train-Test Split\n",
    "\n",
    "Before looking at the text below, try to remember: why is a train-test split the *first* step in a machine learning process?\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer\"><b>Answer (click to reveal)</b></summary>\n",
    "\n",
    "A machine learning (predictive) workflow fundamentally emphasizes creating *a model that will perform well on unseen data*. We will hold out a subset of our original data as the \"test\" set that will stand in for truly unseen data that the model will encounter in the future.\n",
    "\n",
    "We make this separation as the first step for two reasons:\n",
    "\n",
    "1. Most importantly, we are avoiding *leakage* of information from the test set into the training set. Leakage can lead to inflated metrics, since the model has information about the \"unseen\" data that it won't have about real unseen data. This is why we always want to fit our transformers and models on the training data only, not the full dataset.\n",
    "2. Also, we want to make sure the code we have written will actually work on unseen data. If we are able to transform our test data and evaluate it with our final model, that's a good sign that the same process will work for future data as well.\n",
    "    \n",
    "</details>\n",
    "\n",
    "\n",
    "### Loading the Data\n",
    "\n",
    "In the cell below, we import the pandas library and open the full dataset for you. It has already been formatted and subsetted down to the relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>wc</th>\n",
       "      <th>hc</th>\n",
       "      <th>whr</th>\n",
       "      <th>SBP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>28.76</td>\n",
       "      <td>88</td>\n",
       "      <td>101</td>\n",
       "      <td>87</td>\n",
       "      <td>128.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>27.59</td>\n",
       "      <td>86</td>\n",
       "      <td>110</td>\n",
       "      <td>78</td>\n",
       "      <td>123.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>22.45</td>\n",
       "      <td>72</td>\n",
       "      <td>104</td>\n",
       "      <td>69</td>\n",
       "      <td>90.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>28.16</td>\n",
       "      <td>89</td>\n",
       "      <td>108</td>\n",
       "      <td>82</td>\n",
       "      <td>126.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>25.05</td>\n",
       "      <td>81</td>\n",
       "      <td>108</td>\n",
       "      <td>75</td>\n",
       "      <td>120.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>21</td>\n",
       "      <td>45.15</td>\n",
       "      <td>112</td>\n",
       "      <td>132</td>\n",
       "      <td>85</td>\n",
       "      <td>157.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>24</td>\n",
       "      <td>37.89</td>\n",
       "      <td>96</td>\n",
       "      <td>124</td>\n",
       "      <td>77</td>\n",
       "      <td>124.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>37</td>\n",
       "      <td>33.24</td>\n",
       "      <td>104</td>\n",
       "      <td>108</td>\n",
       "      <td>96</td>\n",
       "      <td>126.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>28</td>\n",
       "      <td>35.68</td>\n",
       "      <td>103</td>\n",
       "      <td>130</td>\n",
       "      <td>79</td>\n",
       "      <td>114.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>18</td>\n",
       "      <td>36.24</td>\n",
       "      <td>113</td>\n",
       "      <td>128</td>\n",
       "      <td>88</td>\n",
       "      <td>119.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age    bmi   wc   hc  whr     SBP\n",
       "0     31  28.76   88  101   87  128.00\n",
       "1     21  27.59   86  110   78  123.33\n",
       "2     23  22.45   72  104   69   90.00\n",
       "3     24  28.16   89  108   82  126.67\n",
       "4     20  25.05   81  108   75  120.00\n",
       "..   ...    ...  ...  ...  ...     ...\n",
       "219   21  45.15  112  132   85  157.00\n",
       "220   24  37.89   96  124   77  124.67\n",
       "221   37  33.24  104  108   96  126.67\n",
       "222   28  35.68  103  130   79  114.67\n",
       "223   18  36.24  113  128   88  119.67\n",
       "\n",
       "[224 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/blood_pressure.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Features and Target\n",
    "\n",
    "Once the data is loaded into a pandas dataframe, the next step is identifying which columns represent features and which column represents the target.\n",
    "\n",
    "Recall that in this instance, we are trying to predict systolic blood pressure.\n",
    "\n",
    "In the cell below, assign `X` to be the features and `y` to be the target. Remember that `X` should **NOT** contain the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>wc</th>\n",
       "      <th>hc</th>\n",
       "      <th>whr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>28.76</td>\n",
       "      <td>88</td>\n",
       "      <td>101</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>27.59</td>\n",
       "      <td>86</td>\n",
       "      <td>110</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>22.45</td>\n",
       "      <td>72</td>\n",
       "      <td>104</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>28.16</td>\n",
       "      <td>89</td>\n",
       "      <td>108</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>25.05</td>\n",
       "      <td>81</td>\n",
       "      <td>108</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>21</td>\n",
       "      <td>45.15</td>\n",
       "      <td>112</td>\n",
       "      <td>132</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>24</td>\n",
       "      <td>37.89</td>\n",
       "      <td>96</td>\n",
       "      <td>124</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>37</td>\n",
       "      <td>33.24</td>\n",
       "      <td>104</td>\n",
       "      <td>108</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>28</td>\n",
       "      <td>35.68</td>\n",
       "      <td>103</td>\n",
       "      <td>130</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>18</td>\n",
       "      <td>36.24</td>\n",
       "      <td>113</td>\n",
       "      <td>128</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age    bmi   wc   hc  whr\n",
       "0     31  28.76   88  101   87\n",
       "1     21  27.59   86  110   78\n",
       "2     23  22.45   72  104   69\n",
       "3     24  28.16   89  108   82\n",
       "4     20  25.05   81  108   75\n",
       "..   ...    ...  ...  ...  ...\n",
       "219   21  45.15  112  132   85\n",
       "220   24  37.89   96  124   77\n",
       "221   37  33.24  104  108   96\n",
       "222   28  35.68  103  130   79\n",
       "223   18  36.24  113  128   88\n",
       "\n",
       "[224 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "X = df.drop(columns=['SBP'], axis=1)\n",
    "y = df['SBP']\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the assert statements pass before moving on to the next step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# X should be a 2D matrix with 224 rows and 5 columns\n",
    "assert X.shape == (224, 5)\n",
    "\n",
    "# y should be a 1D array with 224 values\n",
    "assert y.shape == (224,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Train-Test Split\n",
    "\n",
    "In the cell below, import `train_test_split` from scikit-learn ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)).\n",
    "\n",
    "Then create variables `X_train`, `X_test`, `y_train`, and `y_test` using `train_test_split` with `X`, `y`, and `random_state=2021`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Import the relevant function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create train and test data using random_state=2021\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the assert statements pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "assert X_train.shape == (168, 5)\n",
    "assert X_test.shape == (56, 5)\n",
    "\n",
    "assert y_train.shape == (168,)\n",
    "assert y_test.shape == (56,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build and Evaluate a First Simple Model\n",
    "\n",
    "For our baseline model (FSM), we'll use a `LinearRegression` from scikit-learn ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)).\n",
    "\n",
    "### Instantiating the Model\n",
    "\n",
    "In the cell below, instantiate a `LinearRegression` model and assign it to the variable `baseline_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Import the relevant class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Instantiate a linear regression model\n",
    "baseline_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the assert passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# baseline_model should be a linear regression model\n",
    "assert type(baseline_model) == LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are getting the type of `baseline_model` as `abc.ABCMeta`, make sure you actually invoked the constructor of the linear regression class with `()`.\n",
    "\n",
    "If you are getting `NameError: name 'LinearRegression' is not defined`, make sure you have the correct import statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and Evaluating the Model on the Full Training Set\n",
    "\n",
    "In the cell below, fit the model on `X_train` and `y_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, evaluate the model using root mean squared error (RMSE). To do this, first import the `mean_squared_error` function from scikit-learn ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)). Then pass in both the actual and predicted y values, along with `squared=False` (to get the RMSE rather than MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.97633456376879"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Import the relevant function\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate predictions using baseline_model and X_train\n",
    "y_pred_baseline = baseline_model.predict(X_train)\n",
    "\n",
    "# Evaluate using mean_squared_error with squared=False\n",
    "baseline_rmse = mean_squared_error(y_train, y_pred_baseline, squared=False)\n",
    "baseline_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your RMSE calculation should be around 15.98:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "assert round(baseline_rmse, 2) == 15.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that on the *training* data, our predictions are off by about 16 mmHg on average.\n",
    "\n",
    "But what about on *unseen* data?\n",
    "\n",
    "To stand in for true unseen data (and avoid making decisions based on this particular data split, therefore not using `X_test` or `y_test` yet), let's use cross-validation.\n",
    "\n",
    "### Fitting and Evaluating the Model with Cross Validation\n",
    "\n",
    "In the cell below, import `cross_val_score` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)) and call it with `baseline_model`, `X_train`, and `y_train`.\n",
    "\n",
    "For specific implementation reasons within the scikit-learn library, you'll need to use `scoring=\"neg_root_mean_squared_error\"`, which returns the RMSE values with their signs flipped to negative. Then we take the average and negate it at the end, so the number is directly comparable to the RMSE number above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.953844849875598"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Import the relevant function\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Get the cross validated scores for our baseline model\n",
    "baseline_cv = cross_val_score(baseline_model, X_train, y_train, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Display the average of the cross-validated scores\n",
    "baseline_cv_rmse = -(baseline_cv.mean())\n",
    "baseline_cv_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The averaged RMSE for the cross-validated scores should be around 15.95:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "assert round(baseline_cv_rmse, 2) == 15.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Baseline Model\n",
    "\n",
    "So, we got an RMSE of about 16 for both the training data and the validation data. RMSE is a form of *error*, so this means the performance is somewhat better on the validation data than the training data. (This is a bit unusual — normally we expect to see better scores on the training data, but maybe there are some outliers or other reasons that this particular split has this result.)\n",
    "\n",
    "Referring back to the chart above, both errors mean that on average we would expect to mix up someone with stage 1 vs. stage 2 hypertension, but not someone with normal blood pressure vs. critical hypertension. So it appears that the features we have might be predictive enough to be useful.\n",
    "\n",
    "Are we overfitting? Underfitting?\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer\"><b>Answer (click to reveal)</b></summary>\n",
    "\n",
    "The RMSE values for the training data and test data are fairly close to each other and the validation score is actually slightly better than the training score, so we can assume that we are not overfitting.\n",
    "\n",
    "It seems like our model has some room for improvement, but without further investigation it's impossible to know whether we are underfitting, or there is just irreducible error present. Maybe we are simply missing the features we would need to reduce error. (For example, we don't know anything about the diets of these study participants, and we know that diet can influence blood pressure.) But it's also possible that there is some reducible error, meaning we are currently underfitting.\n",
    "\n",
    "In the next step, we'll assume we *are* underfitting, and will attempt to reduce that underfitting by applying some polynomial features transformations to the data.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use `PolynomialFeatures` to Reduce Underfitting\n",
    "\n",
    "Comprehension check: does \"underfitting\" mean we have high *bias*, or high *variance*?\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer\"><b>Answer (click to reveal)</b></summary>\n",
    "\n",
    "Underfitting means high bias. While it's possible that your model will have both high bias and high variance at the same time, in general underfitting means that there is additional information in the data that your model currently isn't picking up on, so you are getting higher error metrics than necessary.\n",
    "    \n",
    "</details>\n",
    "\n",
    "In some model algorithms (e.g. k-nearest neighbors) there are hyperparameters we can adjust so that the model is more flexible and can pick up on additional information in the data. In this case, since we are using linear regression, let's instead perform some feature engineering with `PolynomialFeatures`.\n",
    "\n",
    "### Creating `PolynomialFeatures` Transformer, Fitting and Transforming `X_train`\n",
    "\n",
    "In the cell below, instantiate a `PolynomialFeatures` transformer with default arguments (i.e. just `PolynomialFeatures()`). Documentation for `PolynomialFeatures` can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html).\n",
    "\n",
    "Then fit the transformer on `X_train` and create a new `X_train_poly` matrix by transforming `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Import the relevant class\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Instantiate polynomial features transformer\n",
    "poly = PolynomialFeatures()\n",
    "\n",
    "# Fit transformer on entire X_train\n",
    "poly.fit(X_train)\n",
    "\n",
    "# Create transformed data matrix by transforming X_train\n",
    "X_train_poly = poly.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that `poly` was instantiated correctly, and `X_train_poly` has the correct shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "assert type(poly) == PolynomialFeatures\n",
    "\n",
    "assert X_train_poly.shape == (168, 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and Evaluating the Model on the Transformed Training Set\n",
    "\n",
    "In the cell below, fit the `baseline_model` on `X_train_poly` and `y_train`, then find the RMSE using the same technique you used in Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Fit baseline_model\n",
    "baseline_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_poly = None\n",
    "\n",
    "# Find the RMSE on the full X_train_poly and y_train\n",
    "poly_rmse = None\n",
    "poly_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new RMSE should be about 15.07:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "assert round(poly_rmse, 2) == 15.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and Evaluating the Model with Cross Validation\n",
    "\n",
    "In the cell below, use `cross_val_score` to find an averaged cross-validated RMSE using the same technique you used in Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Get the cross validated scores for our transformed features\n",
    "poly_cv = None\n",
    "\n",
    "# Display the average of the cross-validated scores\n",
    "poly_cv_rmse = -(poly_cv.mean())\n",
    "poly_cv_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validated RMSE should be about 17.74:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "assert round(poly_cv_rmse, 2) == 17.74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of `PolynomialFeatures` Transformation\n",
    "\n",
    "The cell below displays the baseline and transformed values for the full training set vs. the cross-validated average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "print(\"Baseline Model\")\n",
    "print(\"Train RMSE:\", baseline_rmse)\n",
    "print(\"Validation RMSE:\", baseline_cv_rmse)\n",
    "print()\n",
    "print(\"Model with Polynomial Transformation\")\n",
    "print(\"Train RMSE:\", poly_rmse)\n",
    "print(\"Validation RMSE:\", poly_cv_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what does this mean about the result of our polynomial features transformation? What was the impact on bias (underfitting)? What was the impact on variance (overfitting)?\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer\"><b>Answer (click to reveal)</b></summary>\n",
    "\n",
    "The polynomial features transformation did successfully reduce bias (reduce underfitting). We can tell because the RMSE decreased on the training dataset. However, it also increased variance (increased overfitting). We can tell because the RMSE increased on the validation dataset compared to the train dataset.\n",
    "\n",
    "Essentially this means that the polynomial features transformation gave our model the ability to pick up on more information from the training dataset, but some of that information was actually \"noise\" and not information that was useful for making predictions on unseen data.\n",
    "    \n",
    "</details>\n",
    "\n",
    "In the cell below, we plot the train vs. validation RMSE across various different degrees of `PolynomialFeatures`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Create lists of RMSE values\n",
    "train_rmse = []\n",
    "val_rmse = []\n",
    "\n",
    "# Create list of degrees we want to consider\n",
    "degrees = list(range(1,8))\n",
    "\n",
    "for degree in degrees:\n",
    "    # Create transformer of relevant degree and transform X_train\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    baseline_model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    # RMSE for training data\n",
    "    y_pred_poly = baseline_model.predict(X_train_poly)\n",
    "    train_rmse.append(mean_squared_error(y_train, y_pred_poly, squared=False))\n",
    "    \n",
    "    # RMSE for validation data\n",
    "    poly_cv = cross_val_score(baseline_model, X_train_poly, y_train, scoring=\"neg_root_mean_squared_error\")\n",
    "    val_rmse.append(-(poly_cv.mean()))\n",
    "\n",
    "# Set up plot\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(13,5))\n",
    "\n",
    "# Plot RMSE for training data\n",
    "ax1.plot(degrees, train_rmse)\n",
    "ax1.set_title(\"Training Data\")\n",
    "\n",
    "# Plot RMSE for validation data\n",
    "ax2.plot(degrees, val_rmse, color=\"orange\")\n",
    "ax2.set_title(\"Validation Data\")\n",
    "\n",
    "# Shared attributes for plots\n",
    "for ax in (ax1, ax2):\n",
    "    ax.set_xticks(degrees)\n",
    "    ax.set_xlabel(\"Polynomial Degree\")\n",
    "    ax.set_ylabel(\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above graphs, let's plan to use a polynomial degree of 5. Why? Because that is where the RMSE for the training data has dropped down to essentially zero, meaning we are close to perfectly overfitting on the training data.\n",
    "\n",
    "(This is a design decision where there isn't always a single right answer. Later we will introduce a tool called \"grid search\" that will allow you to tune multiple aspects of the model at once instead of having to choose one step at a time like this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Create transformer of relevant degree and transform X_train\n",
    "poly = PolynomialFeatures(5)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "baseline_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# RMSE for training data\n",
    "y_pred_poly = baseline_model.predict(X_train_poly)\n",
    "final_poly_rmse = mean_squared_error(y_train, y_pred_poly, squared=False)\n",
    "\n",
    "# RMSE for validation data\n",
    "poly_cv = cross_val_score(baseline_model, X_train_poly, y_train, scoring=\"neg_root_mean_squared_error\")\n",
    "final_poly_cv_rmse = -(poly_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "print(\"Baseline Model\")\n",
    "print(\"Train RMSE:\", baseline_rmse)\n",
    "print(\"Validation RMSE:\", baseline_cv_rmse)\n",
    "print()\n",
    "print(\"Model with Polynomial Transformation (Degree 5)\")\n",
    "print(\"Train RMSE:\", final_poly_rmse)\n",
    "print(\"Validation RMSE:\", final_poly_cv_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dramatically improved train RMSE (approximately 16 down to 0) and a dramatically worsened validation RMSE (approximately 16 up to 17,000). At this point we are clearly overfitting, but we have successfully reduced the underfitting on the training dataset.\n",
    "\n",
    "In the next step, let's apply a technique to address this overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use Regularization to Reduce Overfitting\n",
    "\n",
    "Let's use regularization to address this overfitting, specifically using the `Ridge` model from scikit-learn ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)), which uses the L2 norm.\n",
    "\n",
    "### Scaling the Data\n",
    "\n",
    "Because L2 regularization is distance-based, we need to scale our data before passing it into this model. In the cell below, instantiate a `StandardScaler` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)) and fit then transform the full `X_train_poly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Import the relevant class\n",
    "None\n",
    "\n",
    "# Instantiate the scaler\n",
    "scaler = None\n",
    "\n",
    "# Fit the scaler on X_train_poly\n",
    "None\n",
    "\n",
    "# Transform the data and create a new matrix\n",
    "X_train_scaled = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaled data should have the same shape as `X_train_poly` but the values should be different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "assert X_train_scaled.shape == X_train_poly.shape\n",
    "assert X_train_scaled[0][0] != X_train_poly[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Ridge Model\n",
    "\n",
    "In the cell below, instantiate a `Ridge` model with `random_state=42`, then fit it on `X_train_scaled` and `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Import the relevant class\n",
    "None\n",
    "\n",
    "# Instantiate the model with random_state=42\n",
    "ridge_model = None\n",
    "\n",
    "# Fit the model\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for Ridge Model\n",
    "\n",
    "Now, find the train and cross-validated RMSE values, and assign them to `ridge_rmse` and `ridge_cv_rmse` respectively. You can refer back to previous steps to remember how to do this! Remember to use `ridge_model` and `X_train_scaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "print(\"Train RMSE:\", ridge_rmse)\n",
    "print(\"Validation RMSE:\", ridge_cv_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your train RMSE should be about 15.24, and validation RMSE should be about 16.05:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "assert round(ridge_rmse, 2) == 15.24\n",
    "assert round(ridge_cv_rmse, 2) == 16.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Model with Regularization\n",
    "\n",
    "The following cell shows metrics for each model so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "print(\"Baseline Model\")\n",
    "print(\"Train RMSE:\", baseline_rmse)\n",
    "print(\"Validation RMSE:\", baseline_cv_rmse)\n",
    "print()\n",
    "print(\"Model with Polynomial Transformation (Degree 5)\")\n",
    "print(\"Train RMSE:\", final_poly_rmse)\n",
    "print(\"Validation RMSE:\", final_poly_cv_rmse)\n",
    "print()\n",
    "print(\"Model with Polynomial Transformation + Regularization\")\n",
    "print(\"Train RMSE:\", ridge_rmse)\n",
    "print(\"Validation RMSE:\", ridge_cv_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we successfully reduce overfitting? Which model is the best model so far?\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer\"><b>Answer (click to reveal)</b></summary>\n",
    "\n",
    "Compared to the model with the polynomial transformation, yes, we successfully reduced overfitting. We can tell because the gap between the train and validation RMSE got a lot smaller.\n",
    "\n",
    "At this point, our best model is actually still the baseline model. Even though we have a lower RMSE for the training data with both the model with polynomial transformation and the model with regularization added, the validation RMSE was still lowest for the baseline model.\n",
    "    \n",
    "</details>\n",
    "\n",
    "Let's try adding stronger regularization penalties, to see if we can reduce the overfitting a bit further while still keeping the improvements to underfitting that we got from the polynomial features transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Create lists of RMSE values\n",
    "train_rmse = []\n",
    "val_rmse = []\n",
    "\n",
    "# Create list of alphas we want to consider\n",
    "alphas = [1, 10, 25, 50, 75, 100, 125, 250, 500]\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Fit a model with a given regularization penalty\n",
    "    model = Ridge(random_state=42, alpha=alpha)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # RMSE for training data\n",
    "    y_pred_ridge = model.predict(X_train_scaled)\n",
    "    train_rmse.append(mean_squared_error(y_train, y_pred_ridge, squared=False))\n",
    "    \n",
    "    # RMSE for validation data\n",
    "    ridge_cv = cross_val_score(model, X_train_scaled, y_train, scoring=\"neg_root_mean_squared_error\")\n",
    "    val_rmse.append(-(ridge_cv.mean()))\n",
    "\n",
    "# Plot train vs. validation RMSE\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(alphas, train_rmse, label=\"Training Data\")\n",
    "ax.plot(alphas, val_rmse, label=\"Validation Data\")\n",
    "ax.set_xlabel(\"Alpha (Regularization Penalty)\")\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This time both are plotted on the same axes because the RMSE has the same order of magnitude.)\n",
    "\n",
    "As we increase the alpha (regularization penalty) along the x-axis, first we can see a big drop in the validation RMSE, then as we keep penalizing more, eventually the RMSE for both the training and validation data starts increasing (meaning we are starting to underfit again).\n",
    "\n",
    "The code below finds the best alpha value from our list, i.e. the alpha that results in the lowest RMSE for the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "lowest_rmse = min(val_rmse)\n",
    "print(\"Lowest RMSE:\", lowest_rmse)\n",
    "\n",
    "best_alpha = alphas[val_rmse.index(lowest_rmse)]\n",
    "print(\"Best alpha:\", best_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a final model using that alpha value and compare it to our previous models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Fit a model with a given regularization penalty\n",
    "final_model = Ridge(random_state=42, alpha=best_alpha)\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# RMSE for training data\n",
    "y_pred_final = final_model.predict(X_train_scaled)\n",
    "final_rmse = mean_squared_error(y_train, y_pred_final, squared=False)\n",
    "\n",
    "# RMSE for validation data\n",
    "final_cv = cross_val_score(final_model, X_train_scaled, y_train, scoring=\"neg_root_mean_squared_error\")\n",
    "final_cv_rmse = -(final_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "print(\"Baseline Model\")\n",
    "print(\"Train RMSE:\", baseline_rmse)\n",
    "print(\"Validation RMSE:\", baseline_cv_rmse)\n",
    "print()\n",
    "print(\"Model with Polynomial Transformation (Degree 5)\")\n",
    "print(\"Train RMSE:\", final_poly_rmse)\n",
    "print(\"Validation RMSE:\", final_poly_cv_rmse)\n",
    "print()\n",
    "print(\"Final Model with Polynomial Transformation + Regularization\")\n",
    "print(\"Train RMSE:\", final_rmse)\n",
    "print(\"Validation RMSE:\", final_cv_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a Final Model\n",
    "\n",
    "While we have already labeled a model as `final_model` above, make sure you understand why: this is the model with the best (lowest) validation RMSE. We also improved the train RMSE somewhat as well, meaning that our modeling strategy has actually reduced both underfitting and overfitting!\n",
    "\n",
    "The impact of the changes made so far has been minimal, which makes sense given our business context. We are trying to predict blood pressure based on proxy measurements that leave out a lot of important information! But we still did see some improvement over the baseline by applying polynomial feature transformation and regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate a Final Model on the Test Set\n",
    "\n",
    "Often our lessons leave out this step because we are focused on other concepts, but if you were to present your final model to stakeholders, it's important to perform one final analysis on truly unseen data to make sure you have a clear idea of how the model will perform in the field.\n",
    "\n",
    "### Instantiating the Final Model\n",
    "\n",
    "Unless you are using a model that is very slow to fit, it's a good idea to re-create it from scratch prior to the final evaluation. That way you avoid any artifacts of how you iterated on the model previously.\n",
    "\n",
    "In the cell below, instantiate a `Ridge` model with `random_state=42` and `alpha=100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "final_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Final Model on the Training Data\n",
    "\n",
    "You can go ahead and use the `X_train_scaled` and `y_train` data we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Test Set\n",
    "\n",
    "The training data for our final model was transformed in two ways:\n",
    "\n",
    "1. Polynomial features added by the `poly` transformer object\n",
    "2. Scaled by the `scaler` transformer object\n",
    "\n",
    "In the cell below, transform the test data in the same way, with the same transformer objects. Do NOT re-instantiate or re-fit these objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Add polynomial features\n",
    "X_test_poly = None\n",
    "\n",
    "# Scale data\n",
    "X_test_scaled = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the shape is correct. If you have too few columns, make sure that you passed the transformed version of `X_test` (`X_test_poly`) to the scaler rather than just `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "assert X_test_scaled.shape == (56, 252)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating RMSE with Final Model and Preprocessed Test Set\n",
    "\n",
    "This time we don't need to use cross-validation, since we are using the test set. In the cell below, generate predictions for the test data then use `mean_squared_error` with `squared=False` to find the RMSE for our holdout test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_test = None\n",
    "\n",
    "# Find RMSE\n",
    "test_rmse = None\n",
    "test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Our Results\n",
    "\n",
    "So, we successfully used polynomial features transformation and regularization to improve our metrics. But, can we recommend that this model be used for the purpose of predicting blood pressure based on these features?\n",
    "\n",
    "Let's create a scatter plot of actual vs. predicted blood pressure, with the boundaries of high blood pressure indicated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plot\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "# Seaborn scatter plot with best fit line\n",
    "sns.regplot(x=y_test, y=y_pred_test, ci=None, truncate=False, ax=ax)\n",
    "ax.set_xlabel(\"Actual Blood Pressure\")\n",
    "ax.set_ylabel(\"Predicted Blood Pressure\")\n",
    "\n",
    "# Add spans showing high blood pressure + legend\n",
    "ax.axvspan(129, max(y_test) + 1, alpha=0.2, color=\"blue\", label=\"actual high blood pressure risk\")\n",
    "ax.axhspan(129, max(y_pred_test) + 1, alpha=0.2, color=\"gray\", label=\"predicted high blood pressure risk\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, as the true blood pressure values increase, so do the predicted blood pressure values. So, it's clear that our model is picking up on *some* information from our features.\n",
    "\n",
    "But it looks like this model does not actually solve the initial business problem very well. Recall that our question was: ***can we predict blood pressure using just a scale and a tape measure?*** Our model would incorrectly flag one person as being at risk of high blood pressure, while missing all of the people who actually are at risk of high blood pressure.\n",
    "\n",
    "It is possible that some other model algorithm (e.g. k-nearest neighbors or decision trees) would do a better job of picking up on the underlying patterns in this dataset. Or if we set this up as a classification problem rather than a regression problem, if we're only interested in flagging high blood pressure rather than predicting blood pressure in general.\n",
    "\n",
    "But if we had to stop this analysis now in its current state, we would need to conclude that **while we were able to pick up some information about blood pressure using these variables alone, we did not produce a model that would work for this business case**.\n",
    "\n",
    "This is something that happens sometimes — not every target can be predicted with the features you have been given! In this case, maybe your model would still be useful for epidemiological modeling (predicting the blood pressure in populations) rather than predicting blood pressure for an individual, since we are picking up on some information. Further study would be needed to determine the feasibility of this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this cumulative lab, you performed an end-to-end machine learning process with correct usage of training, validation, and test data. You identified underfitting and overfitting and applied strategies to address them. Finally, you evaluated your final model using test data, and interpreted those results in the context of a business problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
