{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Text Classification - Cumulative Lab\n", "\n", "## Introduction\n", "\n", "In this cumulative lab, we'll use everything we've learned so far to build a model that can classify a text document as one of many possible classes!\n", "\n", "## Objectives\n", "\n", "You will be able to:\n", "\n", "- Practice cleaning and exploring a text dataset with NLTK and base Python\n", "- Practice using scikit-learn vectorizers for text preprocessing\n", "- Tune a modeling process through exploration and model evaluation\n", "- Observe some techniques for feature engineering\n", "- Interpret the result of a final ML model that classifies text data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Your Task: Complete an End-to-End ML Process with the Newsgroups Dataset\n", "\n", "<a title=\"Bundesarchiv, B 145 Bild-F077948-0006 / Engelbert Reineke / CC-BY-SA 3.0, CC BY-SA 3.0 DE &lt;https://creativecommons.org/licenses/by-sa/3.0/de/deed.en&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Bundesarchiv_B_145_Bild-F077948-0006,_Jugend-Computerschule_mit_IBM-PC.jpg\"><img width=\"512\" alt=\"Bundesarchiv B 145 Bild-F077948-0006, Jugend-Computerschule mit IBM-PC\" src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/Bundesarchiv_B_145_Bild-F077948-0006%2C_Jugend-Computerschule_mit_IBM-PC.jpg\"></a>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Business Understanding\n", "\n", "The ***Newsgroups Dataset*** is a collection of [newsgroup](https://en.wikipedia.org/wiki/Usenet_newsgroup) posts originally collected around 1995. While the backend code implementation is fairly different, you can think of them as like the Reddit posts of 1995, where a \"category\" in this dataset is like a subreddit.\n", "\n", "The task is to try to identify the category where a post was published, based on the text content of the post.\n", "\n", "### Data Understanding\n", "\n", "#### Data Source\n", "\n", "Part of what you are practicing here is using the `sklearn.datasets` submodule, which you have seen before (e.g. the Iris Dataset, the Wine Dataset). You can see a full list of available dataset loaders [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets).\n", "\n", "In this case we will be using the `fetch_20newsgroups` function ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html)). An important thing to note is that because this is text data, scikit-learn actually downloads a set of documents to the computer you are using to complete this lab, rather than just loading data into memory in Python.\n", "\n", "#### Features\n", "\n", "Prior to preprocessing, every row in the dataset only contains one feature: a string containing the full text of the newsgroup post. We will perform preprocessing to create additional features.\n", "\n", "#### Target\n", "\n", "As you might have guessed based on the function name, there are 20 categories in the full dataset. Here is a list of all the possible classes:\n", "\n", "<img src='classes.png'>\n", "\n", "This full dataset is quite large. To save us from extremely long runtimes, we'll work with only a subset of the classes. For this lab, we'll work with the following five:\n", "\n", "* `'comp.windows.x'`\n", "* `'rec.sport.hockey'`\n", "* `'misc.forsale'`\n", "* `'sci.crypt'`\n", "* `'talk.politics.misc'`"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Requirements\n", "\n", "#### 1. Load the Data\n", "\n", "Use pandas and `sklearn.datasets` to load the train and test data into appropriate data structures. Then get a sense of what is in this dataset by visually inspecting some samples.\n", "\n", "#### 2. Perform Data Cleaning and Exploratory Data Analysis with `nltk`\n", "\n", "Standardize the case of the data and use a tokenizer to convert the full posts into lists of individual words. Then compare the raw word frequency distributions of each category.\n", "\n", "#### 3. Build and Evaluate a Baseline Model with `TfidfVectorizer` and `MultinomialNB`\n", "\n", "Ultimately all data must be in numeric form in order to be able to fit a scikit-learn model. So we'll use a tool from `sklearn.feature_extraction.text` to convert all data into a vectorized format.\n", "\n", "Initially we'll keep all of the default parameters for both the vectorizer and the model, in order to develop a baseline score.\n", "\n", "#### 4. Iteratively Perform and Evaluate Preprocessing and Feature Engineering Techniques\n", "\n", "Here you will investigate three techniques, to determine whether they should be part of our final modeling process:\n", "\n", "1. Removing stopwords\n", "2. Using custom tokens\n", "3. Domain-specific feature engineering\n", "4. Increasing `max_features`\n", "\n", "#### 5. Evaluate a Final Model on the Test Set\n", "\n", "Once you have chosen a final modeling process, fit it on the full training data and evaluate it on the test data. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Load the Data\n", "\n", "In the cell below, create the variables `newsgroups_train` and `newsgroups_test` by calling the `fetch_20newsgroups` function twice.\n", "\n", "For the train set, specify `subset=\"train\"`. For the test set, specify `subset=\"test\"`.\n", "\n", "Additionally, pass in `remove=('headers', 'footers', 'quotes')` in both function calls, in order to automatically remove some metadata that can lead to overfitting.\n", "\n", "Recall that we are loading only five categories, out of the full 20. So, pass in `categories=categories` both times."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "from sklearn.datasets import fetch_20newsgroups\n", "\n", "categories = [\n", "    'comp.windows.x',\n", "    'rec.sport.hockey',\n", "    'misc.forsale',\n", "    'sci.crypt',\n", "    'talk.politics.misc'\n", "]\n", "\n", "newsgroups_train = fetch_20newsgroups(\n", "    subset=None,\n", "    remove=None,\n", "    categories=None\n", ")\n", "\n", "newsgroups_test = fetch_20newsgroups(\n", "    subset=None,\n", "    remove=None,\n", "    categories=None\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Each of the returned objects is a dictionary-like `Bunch` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html)):"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "type(newsgroups_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The important thing to know is that the `.data` attribute will extract the feature values, and the `.target` attribute will extract the target values. So, for example, the train features (`X_train`) are located in `newsgroups_train.data`, whereas the train targets (`y_train`) are located in `newsgroups_train.target`.\n", "\n", "In the cell below, create `X_train`, `X_test`, `y_train`, `y_test` based on `newsgroups_train` and `newsgroups_test`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "import pandas as pd\n", "pd.set_option('max_colwidth', 400)\n", "pd.set_option('use_mathjax', False)\n", "\n", "# Extract values from Bunch objects\n", "X_train = pd.DataFrame(None, columns=[\"text\"])\n", "X_test = pd.DataFrame(None, columns=[\"text\"])\n", "y_train = pd.Series(None, name=\"category\")\n", "y_test = pd.Series(None, name=\"category\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Double-check that your variables have the correct shape below:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# X_train and X_test both have 1 column (text)\n", "assert X_train.shape[1] == X_test.shape[1] and X_train.shape[1] == 1\n", "\n", "# y_train and y_test are 1-dimensional (target value only)\n", "assert len(y_train.shape) == len(y_test.shape) and len(y_train.shape) == 1\n", "\n", "# X_train and y_train have the same number of rows\n", "assert X_train.shape[0] == y_train.shape[0] and X_train.shape[0] == 2838\n", "\n", "# X_test and y_test have the same number of rows\n", "assert X_test.shape[0] == y_test.shape[0] and X_test.shape[0] == 1890"]}, {"cell_type": "markdown", "metadata": {}, "source": ["And now let's look at some basic attributes of the dataset.\n", "\n", "#### Distribution of Target\n", "\n", "We know that there are five categories represented. How many are there of each?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "train_target_counts = pd.DataFrame(y_train.value_counts())\n", "train_target_counts[\"label\"] = [newsgroups_train.target_names[val] for val in train_target_counts.index]\n", "train_target_counts.columns = [\"count\", \"target name\"]\n", "train_target_counts.index.name = \"target value\"\n", "train_target_counts"]}, {"cell_type": "markdown", "metadata": {}, "source": ["So, for example, the category \"comp.windows.x\" has the label of `0` in our dataset, and there are 593 text samples in that category within our training data.\n", "\n", "We also note that our target distribution looks reasonably balanced. Now let's look at the features.\n", "\n", "#### Visually Inspecting Features\n", "\n", "Run the cell below to view some examples of the features:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# Sample 5 records and display full text of each\n", "train_sample = X_train.sample(5, random_state=22)\n", "train_sample[\"label\"] = [y_train[val] for val in train_sample.index]\n", "train_sample.style.set_properties(**{'text-align': 'left'})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In order, we have:\n", "\n", "* An example of `comp.windows.x`, talking about \"host loading considerations\"\n", "* An example of `talk.politics.misc`, talking about government and currency\n", "* An example of `misc.forsale`, talking about a list of comics for sale\n", "* An example of `rec.sport.hockey`, talking about hockey players and the Bruins\n", "* An example of `sci.crypt`, talking about a microprocessor\n", "\n", "We appear to have loaded the data correctly, so let's move on and perform some cleaning and additional exploratory analysis."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Perform Data Cleaning and Exploratory Data Analysis with `nltk`\n", "\n", "Prior to any exploratory analysis, we'll complete two common data cleaning tasks for text data: standardizing case and tokenizing.\n", "\n", "### Standardizing Case\n", "\n", "In an NLP modeling process, sometimes we will want to preserve the original case of words (i.e. to treat `\"It\"` and `\"it\"` as different words, and sometimes we will want to standardize case (i.e. to treat `\"It\"` and `\"it\"` as the same word).\n", "\n", "To figure out what we want to do, let's look at the first sample from above:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "windows_sample = train_sample.iloc[0][\"text\"]\n", "windows_sample"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here we have two references to the company Network Computing Devices, or NCD. At the beginning, the poster refers to it as `\"Ncd\"`. Then later refers to `\"support@ncd.com\"`. It seems reasonable to assume that both of these should be treated as references to the same word instead of treating `\"Ncd\"` and `\"ncd\"` as two totally separate things. So let's standardize the case of all letters in this dataset.\n", "\n", "The typical way to standardize case is to make everything lowercase. While it's possible to do this after tokenizing, it's easier and faster to do it first.\n", "\n", "For a single sample, we can just use the built-in Python `.lower()` method:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "windows_sample.lower()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Standarizing Case in the Full Dataset\n", "\n", "To access this method in pandas, you use `.str.lower()`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# Transform sample data to lowercase\n", "train_sample[\"text\"] = train_sample[\"text\"].str.lower()\n", "# Display full text\n", "train_sample.style.set_properties(**{'text-align': 'left'})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the cell below, perform the same operation on the full `X_train`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "# Transform text in X_train to lowercase\n", "None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Double-check your work by looking at an example and making sure the text is lowercase:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "X_train.iloc[100][\"text\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Tokenizing\n", "\n", "Now that the case is consistent it's time to convert each document from a single long string into a set of tokens.\n", "\n", "Let's look more closely at the second example from our training data sample:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "politics_sample = train_sample.iloc[1][\"text\"]\n", "politics_sample"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If we split this into tokens just by using the built-in Python `.split` string method, we would have a lot of punctuation attached:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "politics_sample.split()[:10]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["(Punctuation being attached to words is a problem because we probably want to treat `you` and `you.` as two instances of the same token, not two different tokens.)\n", "\n", "Let's use the default token pattern that scikit-learn uses in its vectorizers. The RegEx looks like this:\n", "\n", "```\n", "(?u)\\b\\w\\w+\\b\n", "```\n", "\n", "That means:\n", "\n", "1. `(?u)`: use full unicode string matching\n", "2. `\\b`: find a word boundary (a word boundary has length 0, and represents the location between non-word characters and word characters)\n", "3. `\\w\\w+`: find 2 or more word characters (all letters, numbers, and underscores are word characters)\n", "4. `\\b`: find another word boundary\n", "\n", "In other words, we are looking for tokens that consist of two or more consecutive word characters, which include letters, numbers, and underscores.\n", "\n", "We'll use the `RegexpTokenizer` from NLTK to create these tokens, initially just transforming the politics sample:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "from nltk.tokenize import RegexpTokenizer\n", "\n", "basic_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n", "\n", "tokenizer = RegexpTokenizer(basic_token_pattern)\n", "tokenizer.tokenize(politics_sample)[:10]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Tokenizing the Full Dataset\n", "\n", "The way to tokenize all values in a column of a pandas dataframe is to use `.apply` and pass in `tokenizer.tokenize`.\n", "\n", "For example, with the sample dataset:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# Create new column with tokenized data\n", "train_sample[\"text_tokenized\"] = train_sample[\"text\"].apply(tokenizer.tokenize)\n", "# Display full text\n", "train_sample.style.set_properties(**{'text-align': 'left'})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the cell below, apply the same operation on `X_train`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "# Create column text_tokenized on X_train\n", "None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Visually inspect your work below:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "X_train.iloc[100][\"text_tokenized\"][:20]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["(Note that we have removed all single-letter words, so instead of `\"have\", \"a\", \"problem\"`, the sample now shows just `\"have\", \"problem\"`. If we wanted to include single-letter words, we could use the token pattern `(?u)\\b\\w+\\b` instead.)\n", "\n", "Now that our data is cleaned up (case standardized and tokenized), we can perform some EDA."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exploratory Data Analysis: Frequency Distributions\n", "\n", "Recall that a frequency distribution is a data structure that contains pieces of data as well as the count of how frequently they appear. In this case, the pieces of data we'll be looking at are tokens (words).\n", "\n", "In the past we have built a frequency distribution \"by hand\" using built-in Python data structures. Here we'll use another handy tool from NLTK called `FreqDist` ([documentation here](http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist)). `FreqDist` allows us to pass in a single list of words, and it produces a dictionary-like output of those words and their frequencies.\n", "\n", "For example, this creates a frequency distribution of the example shown above:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "from nltk import FreqDist\n", "\n", "example_freq_dist = FreqDist(X_train.iloc[100][\"text_tokenized\"][:20])\n", "example_freq_dist"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Then can use Matplotlib to visualize the most common words:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "import matplotlib.pyplot as plt\n", "from matplotlib.ticker import MaxNLocator\n", "\n", "def visualize_top_10(freq_dist, title):\n", "\n", "    # Extract data for plotting\n", "    top_10 = list(zip(*freq_dist.most_common(10)))\n", "    tokens = top_10[0]\n", "    counts = top_10[1]\n", "\n", "    # Set up plot and plot data\n", "    fig, ax = plt.subplots()\n", "    ax.bar(tokens, counts)\n", "\n", "    # Customize plot appearance\n", "    ax.set_title(title)\n", "    ax.set_ylabel(\"Count\")\n", "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n", "    ax.tick_params(axis=\"x\", rotation=90)\n", "    \n", "visualize_top_10(example_freq_dist, \"Top 10 Word Frequency for Example Tokens\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Interpreting the chart above is a bit artificial, since this sample only included 20 tokens. But essentially this is saying that the token with the highest frequency in our example is `\"is\"`, which occurred twice."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Visualizing the Frequency Distribution for the Full Dataset\n", "\n", "Let's do that for the full `X_train`.\n", "\n", "First, we need a list of all of the words in the `text_tokenized` column. We could do this manually by looping over the rows, but fortunately pandas has a handy method called `.explode()` ([documentation here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.explode.html#pandas.Series.explode)) that does exactly this.\n", "\n", "Here is an example applying that to the sample dataframe:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "train_sample[\"text_tokenized\"].explode()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["And we can visualize the top 10 words from the sample dataframe like this:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "sample_freq_dist = FreqDist(train_sample[\"text_tokenized\"].explode())\n", "visualize_top_10(sample_freq_dist, \"Top 10 Word Frequency for 5 Samples\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note that `\"00\"` and `\"50\"` are both in the top 10 tokens, due to many prices appearing in the `misc.forsale` example.\n", "\n", "In the cell below, complete the same process for the full `X_train`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "# Create a frequency distribution for X_train\n", "train_freq_dist = None\n", "\n", "# Plot the top 10 tokens\n", "None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ok great, we have a general sense of the word frequencies in our dataset!\n", "\n", "We can also subdivide this by category, to see if it makes a difference:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# Add in labels for filtering (we won't pass them in to the model)\n", "X_train[\"label\"] = [y_train[val] for val in X_train.index]\n", "\n", "def setup_five_subplots():\n", "    \"\"\"\n", "    It's hard to make an odd number of graphs pretty with just nrows\n", "    and ncols, so we make a custom grid. See example for more details:\n", "    https://matplotlib.org/stable/gallery/subplots_axes_and_figures/gridspec_multicolumn.html\n", "\n", "    We want the graphs to look like this:\n", "     [ ] [ ] [ ]\n", "       [ ] [ ]\n", "\n", "    So we make a 2x6 grid with 5 graphs arranged on it. 3 in the\n", "    top row, 2 in the second row\n", "\n", "      0 1 2 3 4 5\n", "    0|[|]|[|]|[|]|\n", "    1| |[|]|[|]| |\n", "    \"\"\"\n", "    fig = plt.figure(figsize=(15,9))\n", "    fig.set_tight_layout(True)\n", "    gs = fig.add_gridspec(2, 6)\n", "    ax1 = fig.add_subplot(gs[0, :2]) # row 0, cols 0-1\n", "    ax2 = fig.add_subplot(gs[0, 2:4])# row 0, cols 2-3\n", "    ax3 = fig.add_subplot(gs[0, 4:]) # row 0, cols 4-5\n", "    ax4 = fig.add_subplot(gs[1, 1:3])# row 1, cols 1-2\n", "    ax5 = fig.add_subplot(gs[1, 3:5])# row 1, cols 3-4\n", "    return fig, [ax1, ax2, ax3, ax4, ax5]\n", "\n", "def plot_distribution_of_column_by_category(column, axes, title=\"Word Frequency for\"):\n", "    for index, category in enumerate(newsgroups_train.target_names):\n", "        # Calculate frequency distribution for this subset\n", "        all_words = X_train[X_train[\"label\"] == index][column].explode()\n", "        freq_dist = FreqDist(all_words)\n", "        top_10 = list(zip(*freq_dist.most_common(10)))\n", "        tokens = top_10[0]\n", "        counts = top_10[1]\n", "\n", "        # Set up plot\n", "        ax = axes[index]\n", "        ax.bar(tokens, counts)\n", "\n", "        # Customize plot appearance\n", "        ax.set_title(f\"{title} {category}\")\n", "        ax.set_ylabel(\"Count\")\n", "        ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n", "        ax.tick_params(axis=\"x\", rotation=90)\n", "\n", "\n", "fig, axes = setup_five_subplots()\n", "plot_distribution_of_column_by_category(\"text_tokenized\", axes)\n", "fig.suptitle(\"Word Frequencies for All Tokens\", fontsize=24);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If these were unlabeled, would you be able to figure out which one matched with which category?\n", "\n", "Well, `misc.forsale` still has a number (`\"00\"`) as one of its top tokens, so you might be able to figure out that one, but it seems very difficult to distinguish the others; every single category has `\"the\"` as the most common token, and every category except for `misc.forsale` has `\"to\"` as the second most common token. \n", "\n", "After building our baseline model, we'll use this information to inform our next preprocessing steps."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Build and Evaluate a Baseline Model with `TfidfVectorizer` and `MultinomialNB`\n", "\n", "Let's start modeling by building a model that basically only has access to the information in the plots above. So, using the default token pattern to split the full text into tokens, and using a limited vocabulary.\n", "\n", "To give the model a little bit more information with those same features, we'll use a `TfidfVectorizer` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)) so that it counts not only the term frequency (`tf`) within a single document, it also includes the inverse document frequency (`idf`) \u2014 how rare the term is.\n", "\n", "In the cell below, import the vectorizer, instantiate a vectorizer object, and fit it on `X_train[\"text\"]`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "# Import the relevant vectorizer class\n", "None\n", "\n", "# Instantiate a vectorizer with max_features=10\n", "# (we are using the default token pattern)\n", "tfidf = None\n", "\n", "# Fit the vectorizer on X_train[\"text\"] and transform it\n", "X_train_vectorized = None\n", "\n", "# Visually inspect the 10 most common words\n", "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check the shape of your vectorized data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# We should still have the same number of rows\n", "assert X_train_vectorized.shape[0] == X_train.shape[0]\n", "\n", "# The vectorized version should have 10 columns, since we set\n", "# max_features=10\n", "assert X_train_vectorized.shape[1] == 10"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that we have preprocessed data, fit and evaluate a multinomial Naive Bayes classifier ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)) using `cross_val_score` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html))."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "# Import relevant class and function\n", "None\n", "None\n", "\n", "# Instantiate a MultinomialNB classifier\n", "baseline_model = None\n", "\n", "# Evaluate the classifier on X_train_vectorized and y_train\n", "baseline_cv = cross_val_score(None, None, None)\n", "baseline_cv"]}, {"cell_type": "markdown", "metadata": {}, "source": ["How well is this model performing? Well, recall the class balance:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "y_train.value_counts(normalize=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If we guessed the plurality class every time (class `2`), we would expect about 21% accuracy. So when this model is getting 37-42% accuracy, that is a clear improvement over just guessing. But with an accuracy below 50%, we still expect the model to guess the wrong class the majority of the time. Let's see if we can improve that with more sophisticated preprocessing."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Iteratively Perform and Evaluate Preprocessing and Feature Engineering Techniques\n", "\n", "Now that we have our baseline, the fun part begins. As you've seen throughout this section, preprocessing text data is a bit more challenging that working with more traditional data types because there's no clear-cut answer for exactly what sort of preprocessing we need to do. As we are preprocessing our text data, we need to make some decisions about things such as:\n", "\n", "* Do we remove stop words or not?\n", "* What should be counted as a token? Do we stem or lemmatize our text data, or leave the words as is? Do we want to include non-\"words\" in our tokens?\n", "* Do we engineer other features, such as bigrams, or POS tags, or Mutual Information Scores?\n", "* Do we use the entire vocabulary, or just limit the model to a subset of the most frequently used words? If so, how many?\n", "* What sort of vectorization should we use in our model? Boolean Vectorization? Count Vectorization? TF-IDF? More advanced vectorization strategies such as Word2Vec?\n", "\n", "In this lab, we will work through the first four of these."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Removing Stopwords\n", "\n", "Let's begin with the first question: ***do we remove stopwords or not?*** In general we assume that stopwords do not contain useful information, but that is not always the case. Let's empirically investigate the top word frequencies of each category to see whether removing stopwords helps us to distinguish between the catogories.\n", "\n", "As-is, recall that the raw word frequency distributions of 4 out of 5 categories look very similar. They start with `the` as the word with by far the highest frequency, then there is a downward slope of other common words, starting with `to`. The `misc.forsale` category looks a little different, but it still has `the` as the top token.\n", "\n", "If we remove stopwords, how does this change the frequency distributions for each category?\n", "\n", "#### Stopwords List\n", "\n", "Once again, NLTK has a useful tool for this task. You can just import a list of standard stopwords:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "import nltk\n", "nltk.download('stopwords', quiet=True)\n", "from nltk.corpus import stopwords\n", "\n", "stopwords_list = stopwords.words('english')\n", "stopwords_list[:20]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can customize that list as well.\n", "\n", "Let's say that we want to keep the word `\"for\"` in our final vocabulary, since it appears disproportionately often in the `misc.forsale` category. The code below removes that from the stopwords:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "print(\"Original list length:\", len(stopwords_list))\n", "stopwords_list.pop(stopwords_list.index(\"for\"))\n", "print(\"List length after removing 'for':\", len(stopwords_list))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the cell below, write a function `remove_stopwords` that takes in a list-like collection of strings (tokens) and returns only those that are not in the list of stopwords. (Use the `stopwords_list` in the global scope, so that we can later use `.apply` with this function.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "def remove_stopwords(token_list):\n", "    \"\"\"\n", "    Given a list of tokens, return a list where the tokens\n", "    that are also present in stopwords_list have been\n", "    removed\n", "    \"\"\"\n", "    None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Test it out on one example:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "tokens_example = X_train.iloc[100][\"text_tokenized\"]\n", "print(\"Length with stopwords:\", len(tokens_example))\n", "assert len(tokens_example) == 110\n", "\n", "tokens_example_without_stopwords = remove_stopwords(tokens_example)\n", "print(\"Length without stopwords:\", len(tokens_example_without_stopwords))\n", "assert len(tokens_example_without_stopwords) == 65"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If that ran successfully, go ahead and apply it to the full `X_train`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "X_train[\"text_without_stopwords\"] = X_train[\"text_tokenized\"].apply(remove_stopwords)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can compare frequency distributions without stopwords:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "fig, axes = setup_five_subplots()\n", "plot_distribution_of_column_by_category(\"text_without_stopwords\", axes)\n", "fig.suptitle(\"Word Frequencies without Stopwords\", fontsize=24);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ok, this seems to answer our question. The most common words differ significantly between categories now, meaning that hopefully our model will have an easier time distinguishing between them.\n", "\n", "Let's redo our modeling process, using `stopwords_list` when instantiating the vectorizer:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# Instantiate the vectorizer\n", "tfidf = TfidfVectorizer(\n", "    max_features=10,\n", "    stop_words=stopwords_list\n", ")\n", "\n", "# Fit the vectorizer on X_train[\"text\"] and transform it\n", "X_train_vectorized = tfidf.fit_transform(X_train[\"text\"])\n", "\n", "# Visually inspect the vectorized data\n", "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# Evaluate the classifier on X_train_vectorized and y_train\n", "stopwords_removed_cv = cross_val_score(baseline_model, X_train_vectorized, y_train)\n", "stopwords_removed_cv"]}, {"cell_type": "markdown", "metadata": {}, "source": ["How does this compare to our baseline?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "print(\"Baseline:         \", baseline_cv.mean())\n", "print(\"Stopwords removed:\", stopwords_removed_cv.mean())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Looks like we have a marginal improvement, but still an improvement. So, to answer ***do we remove stopwords or not:*** yes, let's remove stopwords."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Using Custom Tokens\n", "\n", "Our next question is ***what should be counted as a token?***\n", "\n", "Recall that currently we are using the default token pattern, which finds words of two or more characters. What happens if we also *stem* those words, so that `swims` and `swimming` would count as the same token?\n", "\n", "Here we have provided a custom tokenizing function:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "from nltk.stem.snowball import SnowballStemmer\n", "stemmer = SnowballStemmer(language=\"english\")\n", "\n", "def stem_and_tokenize(document):\n", "    tokens = tokenizer.tokenize(document)\n", "    return [stemmer.stem(token) for token in tokens]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This uses `tokenizer` that we created earlier, as well as a new `stemmer` object. See an example below:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "print(\"Original sample:\", X_train.iloc[100][\"text_tokenized\"][20:30])\n", "print(\"Stemmed sample: \", stem_and_tokenize(X_train.iloc[100][\"text\"])[20:30])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We also need to stem our stopwords:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "stemmed_stopwords = [stemmer.stem(word) for word in stopwords_list]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the cells below, repeat the modeling process from earlier. This time when instantiating the `TfidfVectorizer`, specify:\n", "\n", "* `max_features=10` (same as previous)\n", "* `stop_words=stemmed_stopwords` (modified)\n", "* `tokenizer=stem_and_tokenize` (new)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "# Instantiate the vectorizer\n", "tfidf = None\n", "\n", "# Fit the vectorizer on X_train[\"text\"] and transform it\n", "X_train_vectorized = tfidf.fit_transform(X_train[\"text\"])\n", "\n", "# Visually inspect the vectorized data\n", "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# Evaluate the classifier on X_train_vectorized and y_train\n", "stemmed_cv = cross_val_score(baseline_model, X_train_vectorized, y_train)\n", "stemmed_cv"]}, {"cell_type": "markdown", "metadata": {}, "source": ["How does this compare to our previous best modeling process?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "print(\"Stopwords removed:\", stopwords_removed_cv.mean())\n", "print(\"Stemmed:          \", stemmed_cv.mean())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Great! Another improvement, a slightly bigger one than we got when just removing stopwords. So, our best modeling process for now is one where we remove stopwords, use the default token pattern, and stem our tokens with a snowball stemmer."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Domain-Specific Feature Engineering\n", "\n", "The way to really get the most information out of text data is by adding features beyond just vectorizing the tokens. This code will be completed for you, and it's okay if you don't fully understand everything that is happening, but we hope it helps you brainstorm for future projects!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Number of Sentences\n", "\n", "Does the number of sentences in a post differ by category? Let's investigate.\n", "\n", "Once again, there is a tool from NLTK that helps with this task."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "from nltk.tokenize import sent_tokenize\n", "\n", "sent_tokenize(X_train.iloc[100][\"text\"])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can just take the length of this list to find the number of sentences:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "len(sent_tokenize(X_train.iloc[100][\"text\"]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The following code adds a feature `num_sentences` to `X_train`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "X_train[\"num_sentences\"] = X_train[\"text\"].apply(lambda x: len(sent_tokenize(x)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "fig, axes = setup_five_subplots()\n", "plot_distribution_of_column_by_category(\"num_sentences\", axes, \"Numbers of Sentences for\")\n", "fig.suptitle(\"Distributions of Sentence Counts by Category\", fontsize=24);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Does this seem like a useful feature? Maybe. The distributions differ a bit, but it's hard to know if our model will pick up on this information. Let's go ahead and keep it."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Contains a Price\n", "\n", "The idea here is particularly to be able to distinguish the `misc.forsale` category, but it might also help with identifying the others. Let's use RegEx to check if the text contains a price:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# Define a price as a dollar sign followed by 1-3 numbers,\n", "# optional commas or decimals, 1-2 numbers after the decimal\n", "# (we're not too worried about accidentally matching malformed prices)\n", "price_query = r'\\$(?:\\d{1,3}[,.]?)+(?:\\\\d{1,2})?'\n", "\n", "X_train[\"contains_price\"] = X_train[\"text\"].str.contains(price_query)\n", "\n", "fig, axes = setup_five_subplots()\n", "plot_distribution_of_column_by_category(\"contains_price\", axes, \"Freqency of Posts Containing Prices for\")\n", "fig.suptitle(\"Distributions of Posts Containing Prices by Category\", fontsize=24);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As we expected, the `misc.forsale` category looks pretty different from the others. More than half of those posts contain prices, whereas the overwhelming majority of posts in other categories do not contain prices. Let's include this in our final model."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Contains an Emoticon\n", "\n", "This is a bit silly, but we were wondering whether different categories feature different numbers of emoticons.\n", "\n", "Here we define an emoticon as an ASCII character representing eyes, an optional ASCII character representing a nose, and an ASCII character representing a mouth."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "emoticon_query = r'(?:[\\:;X=B][-^]?[)\\]3D([OP/\\\\|])(?:(?=\\s))'\n", "\n", "X_train[\"contains_emoticon\"] = X_train[\"text\"].str.contains(emoticon_query)\n", "\n", "fig, axes = setup_five_subplots()\n", "plot_distribution_of_column_by_category(\"contains_emoticon\", axes, \"Freqency of Posts Containing Emoticons for\")\n", "fig.suptitle(\"Distributions of Posts Containing Emoticons by Category\", fontsize=24);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Well, that was a lot less definitive. Emoticons are fairly rare across categories. But, there are some small differences so let's go ahead and keep it."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Modeling with Vectorized Features + Engineered Features \n", "\n", "Let's combine our best vectorizer with these new features:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# Instantiate the vectorizer\n", "tfidf = TfidfVectorizer(\n", "    max_features=10,\n", "    stop_words=stemmed_stopwords,\n", "    tokenizer=stem_and_tokenize\n", ")\n", "\n", "# Fit the vectorizer on X_train[\"text\"] and transform it\n", "X_train_vectorized = tfidf.fit_transform(X_train[\"text\"])\n", "\n", "# Create a full df of vectorized + engineered features\n", "X_train_vectorized_df = pd.DataFrame(X_train_vectorized.toarray(), columns=tfidf.get_feature_names())\n", "preprocessed_X_train = pd.concat([\n", "    X_train_vectorized_df, X_train[[\"num_sentences\", \"contains_price\", \"contains_emoticon\"]]\n", "], axis=1)\n", "preprocessed_X_train"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "preprocessed_cv = cross_val_score(baseline_model, preprocessed_X_train, y_train)\n", "preprocessed_cv"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "print(\"Stemmed:           \", stemmed_cv.mean())\n", "print(\"Fully preprocessed:\", preprocessed_cv.mean())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ok, another small improvement! We're still a bit below 50% accuracy, but we're getting improvements every time."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Increasing `max_features`\n", "\n", "Right now we are only allowing the model to look at the tf-idf of the top 10 most frequent tokens. If we allow it to look at all possible tokens, that could lead to high dimensionality issues (especially if we have more rows than columns), but there is a lot of room between 10 and `len(X_train)` features:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "len(X_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["(In other words, setting `max_features` to 2838 would mean an equal number of rows and columns, something that can cause problems for many model algorithms.)\n", "\n", "Let's try increasing `max_features` from 10 to 200:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "# Instantiate the vectorizer\n", "tfidf = TfidfVectorizer(\n", "    max_features=None,\n", "    stop_words=stemmed_stopwords,\n", "    tokenizer=stem_and_tokenize\n", ")\n", "\n", "# Fit the vectorizer on X_train[\"text\"] and transform it\n", "X_train_vectorized = tfidf.fit_transform(X_train[\"text\"])\n", "\n", "# Create a full df of vectorized + engineered features\n", "X_train_vectorized_df = pd.DataFrame(X_train_vectorized.toarray(), columns=tfidf.get_feature_names())\n", "final_X_train = pd.concat([\n", "    X_train_vectorized_df, X_train[[\"num_sentences\", \"contains_price\", \"contains_emoticon\"]]\n", "], axis=1)\n", "final_X_train"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "final_cv = cross_val_score(baseline_model, final_X_train, y_train)\n", "final_cv"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Nice! Our model was able to learn a lot more with these added features. Let's say this is our final modeling process and move on to a final evaluation."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Evaluate a Final Model on the Test Set\n", "\n", "Instantiate the model, fit it on the full training set and check the score:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "final_model = MultinomialNB()\n", "\n", "final_model.fit(final_X_train, y_train)\n", "final_model.score(final_X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create a vectorized version of `X_test`'s text:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# Note that we just transform, don't fit_transform\n", "X_test_vectorized = tfidf.transform(X_test[\"text\"])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Feature engineering for `X_test`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "X_test[\"num_sentences\"] = X_test[\"text\"].apply(lambda x: len(sent_tokenize(x)))\n", "X_test[\"contains_price\"] = X_test[\"text\"].str.contains(price_query)\n", "X_test[\"contains_emoticon\"] = X_test[\"text\"].str.contains(emoticon_query)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Putting it all together:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "X_test_vectorized_df = pd.DataFrame(X_test_vectorized.toarray(), columns=tfidf.get_feature_names())\n", "final_X_test = pd.concat([\n", "    X_test_vectorized_df, X_test[[\"num_sentences\", \"contains_price\", \"contains_emoticon\"]]\n", "], axis=1)\n", "final_X_test"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Scoring on the test set:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "final_model.score(final_X_test, y_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plotting a confusion matrix:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "from sklearn.metrics import plot_confusion_matrix\n", "plot_confusion_matrix(final_model, final_X_test, y_test);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Recall that these are the names associated with the labels:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "target_values_and_names = train_target_counts.drop(\"count\", axis=1)\n", "target_values_and_names"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Interpreting Results\n", "\n", "Interpret the results seen above. How well did the model do? How does it compare to random guessing? What can you say about the cases that the model was most likely to mislabel? If this were a project and you were describing next steps, what might those be?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate text\n", "\"\"\"\n", "None\n", "\"\"\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Summary\n", "\n", "In this lab, we used our NLP skills to clean, preprocess, explore, and fit models to text data for classification. This wasn't easy \u2014 great job!!"]}], "metadata": {"kernelspec": {"display_name": "Python (learn-env)", "language": "python", "name": "learn-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}